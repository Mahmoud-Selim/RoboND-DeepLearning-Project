# Follow Me Deep Learning Project 

In this project, A deep neural network was trained to identify and track a target in simulation. So-called “follow me” applications like this are key to many fields of robotics and could be extended to scenarios like advanced cruise control in autonomous vehicles or human-robot collaboration in industry.
---
[//]: # (Image References)

[image0]: ./docs/misc/Architecture.PNG

## NEURAL NETWORK ARCHITECTURE

The neural network used in this project was a fully convolutional neural netowrk (FCN) that was doing a semantic segmentation for the objects in the images. the network that was used had two parts. The first one is the encoder and the second part is the decoder. The encoder portion is a separable convolution network that reduces to a deeper 1x1 convolution layer, in contrast to a flat fully connected layer that would be used for basic classification of images. This difference has the effect of preserving spacial information from the image. 
 
### Separable Convolutions 
Separable convolutions, also known as depthwise separable convolutions, comprise of a convolution performed over each channel of an input layer and followed by a 1x1 convolution that takes the output channels from the previous step and then combines them into an output layer. This is different than regular convolutions , mainly because of the reduction in the number of parameters. 

The reduction in the parameters make separable convolutions quite efficient with improved runtime performance and are also, as a result, useful for mobile applications. They also have the added benefit of reducing overfitting to an extent, because of the fewer parameters.

### Batch Normalization

Batch normalization is based on the idea that, instead of just normalizing the inputs to the network, we normalize the inputs to layers within the network. It's called "batch" normalization because during training, we normalize each layer's inputs by using the mean and variance of the values in the current mini-batch.

A network is a series of layers, where the output of one layer becomes the input to another. That means we can think of any layer in a neural network as the first layer of a smaller network.

Batch normalization layer presents us with quite a few advantages. Some of them are:

1. Networks train faster – Each training iteration will actually be slower because of the extra calculations during the forward pass. However, it should converge much more quickly, so training should be faster overall.
2. Allows higher learning rates – Gradient descent usually requires small learning rates for the network to converge. And as networks get deeper, their gradients get smaller during back propagation so they require even more iterations. Using batch normalization allows us to use much higher learning rates, which further increases the speed at which networks train.
3. Simplifies the creation of deeper networks – Because of the above reasons, it is easier to build and faster to train deeper neural networks when using batch normalization.
4. Provides a bit of regularization – Batch normalization adds a little noise to your network. In some cases, such as in Inception modules, batch normalization has been shown to work as well as dropout.

### The Encoder Architecture
The	encoding stage is responsible for extracting image representations (feature maps) at different scales. The different feature maps are produced by the different filters and the variable scales arise from the fact that each encoder halves the spatial extent(height and width) of it's input. The encoder consisted of multiple encoder blocks. Each block has a seperable convolution layer followed by a batch-norm layer

### The Decoder Architecture
The decoder stage uses both the final and intermediate feature-maps generated by the encoder stage (through skip connections) to reconstruct an image (through up-sampling and filtering) with the same size as the original image but, with pixel-wise segmentation. The decoder consisted of multiple encoder blocks. Each block has an upsampling layer followed by a concatenation and 2 seperable convolution layers with batch-norms.

A screenshot of the model is shown below

![alt text][image0]

---

## The Training Process
A simple dataset was provided with the simulator for training. However, i collected new images from 2 runs. Each run had 1000 image each. After that, i processed the images and gone with the training process. The hyper parameter trunning was done as follows:

1. The learning rate was set to .001 to allow for a reliable training process. Didn't go with .01 to prevent any gradient from exploding. Specially that the dataset was small and may very well not be able to represent the distribution of the data well.
2. The batch size was set to be 128. It was choosen so to reduce any noisy steps while utilizing the gpu well.
3. The number of epochs was set to 50. It was enough as it passed the required accuracy
4. the steps per epoch were 50 steps. 
---

## Future Work
The most obvious solutions are to increase the dataset, adjust the architecutre and get rid of the seperable convolution and use a normal one. However, a more relaistic one is to do finetunning. Obviously, it's a tedious process to collect the training and the validation dataset. So, one approach is to train the encoder-decoder on a different images. And after that, we can transfer learn on our small dataset. Another thing is to use multiple encoders and stack their encodings before the decoder part. 